{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de362fd7-c402-440a-8f67-25221733cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#import os\n",
    "\n",
    "# Path to your zip file\n",
    "#zip_file = \"Sentiment_Analysis.zip\"\n",
    "#extract_to = \"Sentiment_Analysis\"  # Folder to extract into\n",
    "\n",
    "# Create folder if not exists\n",
    "#os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "# Unzip the file\n",
    "#with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    " #   zip_ref.extractall(extract_to)\n",
    "\n",
    "#print(\"✅ Dataset extracted to:\", extract_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e191dda-09b3-4d06-bb36-8baccc494e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 6 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n",
      "\n",
      "WARNING: You are on Windows. If you detect any issue with pandarallel, be sure you checked out the Troubleshooting page:\n",
      "https://nalepae.github.io/pandarallel/troubleshooting/\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Lakshatha\n",
      "[nltk_data]     S\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 1.Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "\n",
    "# Initialize pandarallel for faster processing\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "# Download stopwords from NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Create a set of English stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83b5c64a-3105-4115-bc67-4228dbca7c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.Base dataset path (adjust if needed)\n",
    "base_path = \"Sentiment_Analysis/Sentiment_Analysis\"\n",
    "\n",
    "# Paths for train/test/unsupervised folders\n",
    "train_pos = os.path.join(base_path, \"train/pos\")\n",
    "train_neg = os.path.join(base_path, \"train/neg\")\n",
    "train_unsup = os.path.join(base_path, \"train/unsup\")\n",
    "test_pos = os.path.join(base_path, \"test/pos\")\n",
    "test_neg = os.path.join(base_path, \"test/neg\")\n",
    "\n",
    "# Function to load labeled data\n",
    "def load_labeled_reviews(folder, label):\n",
    "    \"\"\"Load all reviews from folder and assign a label.\"\"\"\n",
    "    files = glob.glob(os.path.join(folder, \"*.txt\"))\n",
    "    data = []\n",
    "    for f in files:\n",
    "        with open(f, encoding='utf-8') as file:\n",
    "            data.append((file.read(), label))\n",
    "    return data\n",
    "\n",
    "# Function to load unsupervised (unlabeled) data\n",
    "def load_unsupervised_data(folder):\n",
    "    \"\"\"Load all reviews from folder without labels.\"\"\"\n",
    "    files = glob.glob(os.path.join(folder, \"*.txt\"))\n",
    "    texts = []\n",
    "    for f in files:\n",
    "        with open(f, encoding='utf-8') as file:\n",
    "            texts.append(file.read())\n",
    "    return texts\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    \"\"\"Lowercase, remove HTML, punctuation, numbers, and stopwords.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'<.*?>', '', text)                \n",
    "    text = re.sub(r'[^a-z\\s]', '', text)             \n",
    "    text = re.sub(r'\\s+', ' ', text).strip()         \n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b022fa5c-2fc3-4acf-9035-f1e6427dadd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labeled train and test data...\n",
      "Loading unsupervised train data...\n",
      "Train set size: (25000, 2)\n",
      "Test set size: (25000, 2)\n",
      "Unsupervised samples count: 50000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#3.Load all labeled and unlabeled data\n",
    "print(\"Loading labeled train and test data...\")\n",
    "train_pos_data = load_labeled_reviews(train_pos, 1)\n",
    "train_neg_data = load_labeled_reviews(train_neg, 0)\n",
    "test_pos_data = load_labeled_reviews(test_pos, 1)\n",
    "test_neg_data = load_labeled_reviews(test_neg, 0)\n",
    "\n",
    "print(\"Loading unsupervised train data...\")\n",
    "unsup_texts = load_unsupervised_data(train_unsup)\n",
    "\n",
    "# Combine labeled data into DataFrames\n",
    "train_data = train_pos_data + train_neg_data\n",
    "test_data = test_pos_data + test_neg_data\n",
    "\n",
    "train_df = pd.DataFrame(train_data, columns=[\"review\", \"label\"])\n",
    "test_df = pd.DataFrame(test_data, columns=[\"review\", \"label\"])\n",
    "\n",
    "print(f\"Train set size: {train_df.shape}\")\n",
    "print(f\"Test set size: {test_df.shape}\")\n",
    "print(f\"Unsupervised samples count: {len(unsup_texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68f861e2-b402-4a69-9167-b974c95ed640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning train reviews...\n",
      "Cleaning test reviews...\n",
      "Cleaning unsupervised reviews...\n",
      "Cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "# 4.Clean all datasets\n",
    "# 4. Clean all datasets without pandarallel (portable version)\n",
    "\n",
    "print(\"Cleaning train reviews...\")\n",
    "train_df['cleaned'] = train_df['review'].apply(clean_text)  # Normal apply instead of parallel\n",
    "\n",
    "print(\"Cleaning test reviews...\")\n",
    "test_df['cleaned'] = test_df['review'].apply(clean_text)    # Normal apply\n",
    "\n",
    "print(\"Cleaning unsupervised reviews...\")\n",
    "unsup_cleaned = [clean_text(text) for text in unsup_texts]   # List comprehension\n",
    "\n",
    "print(\"Cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a3dc907-609c-487e-be0b-9413aff4ceef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Word2Vec model on unsupervised data...\n",
      "Word2Vec model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# 5.Train Word2Vec embeddings on unsupervised data\n",
    "print(\"Training Word2Vec model on unsupervised data...\")\n",
    "tokenized_unsup = [text.split() for text in unsup_cleaned]\n",
    "\n",
    "w2v_model = Word2Vec(\n",
    "    sentences=tokenized_unsup,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=5,\n",
    "    workers=os.cpu_count(),\n",
    "    epochs=5\n",
    ")\n",
    "\n",
    "w2v_model.save(\"word2vec_unsup.model\")\n",
    "print(\"Word2Vec model trained and saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0360ef9a-3064-420e-9d9a-fbdf10532b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing tokenizer and sequences...\n",
      "Tokenizer vocab size: 214479\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6.Tokenizer for converting text to numeric sequences\n",
    "print(\"Preparing tokenizer and sequences...\")\n",
    "all_texts = pd.concat([train_df['cleaned'], test_df['cleaned']])\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "# Convert to sequences and pad\n",
    "X_train_seq = tokenizer.texts_to_sequences(train_df['cleaned'])\n",
    "X_test_seq = tokenizer.texts_to_sequences(test_df['cleaned'])\n",
    "\n",
    "max_len = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "# Labels\n",
    "y_train = train_df['label'].values\n",
    "y_test = test_df['label'].values\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer.word_index) + 1}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66b2ebeb-dbeb-4e66-9004-900bfa989cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embedding matrix...\n",
      "Embedding matrix created.\n"
     ]
    }
   ],
   "source": [
    "# 7.Create embedding matrix from Word2Vec model\n",
    "print(\"Creating embedding matrix...\")\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_dim = 100\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, idx in tokenizer.word_index.items():\n",
    "    if word in w2v_model.wv:\n",
    "        embedding_matrix[idx] = w2v_model.wv[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = np.random.normal(size=(embedding_dim,))\n",
    "\n",
    "print(\"Embedding matrix created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a349fe44-ce63-491f-a165-449e0aa4bbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building BiLSTM model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lakshatha S\\.conda\\envs\\py311\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">200</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)            │      <span style=\"color: #00af00; text-decoration-color: #00af00\">21,447,900</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">234,496</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,448</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">65</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m200\u001b[0m, \u001b[38;5;34m100\u001b[0m)            │      \u001b[38;5;34m21,447,900\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m234,496\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m16,448\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │              \u001b[38;5;34m65\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,698,909</span> (82.77 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m21,698,909\u001b[0m (82.77 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">251,009</span> (980.50 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m251,009\u001b[0m (980.50 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">21,447,900</span> (81.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m21,447,900\u001b[0m (81.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 8. Build a BiLSTM model for sentiment classification\n",
    "print(\"Building BiLSTM model...\")\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=vocab_size,\n",
    "              output_dim=embedding_dim,\n",
    "              weights=[embedding_matrix],\n",
    "              input_length=max_len,      # ✅ Added so model knows input size\n",
    "              trainable=False),\n",
    "    Bidirectional(LSTM(128)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Build model explicitly to show correct shapes\n",
    "model.build(input_shape=(None, max_len))  # ✅ This builds the model before summary\n",
    "\n",
    "# Show model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3316c3b2-0e3d-48ad-a2f7-79a1bd5e4817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training initial model...\n",
      "Epoch 1/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m209s\u001b[0m 539ms/step - accuracy: 0.7426 - loss: 0.5316 - val_accuracy: 0.8552 - val_loss: 0.3219 - learning_rate: 0.0010\n",
      "Epoch 2/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m183s\u001b[0m 521ms/step - accuracy: 0.8130 - loss: 0.4368 - val_accuracy: 0.8572 - val_loss: 0.3039 - learning_rate: 0.0010\n",
      "Epoch 3/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m189s\u001b[0m 537ms/step - accuracy: 0.8559 - loss: 0.3445 - val_accuracy: 0.8348 - val_loss: 0.3872 - learning_rate: 0.0010\n",
      "Epoch 4/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m186s\u001b[0m 528ms/step - accuracy: 0.8705 - loss: 0.3154 - val_accuracy: 0.8724 - val_loss: 0.3307 - learning_rate: 0.0010\n",
      "Epoch 5/10\n",
      "\u001b[1m352/352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m191s\u001b[0m 542ms/step - accuracy: 0.8830 - loss: 0.2828 - val_accuracy: 0.8688 - val_loss: 0.3422 - learning_rate: 5.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# 9.Train initial model with callbacks\n",
    "print(\"Training initial model...\")\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "417c29dd-2a87-4314-a444-3a3095ef0d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semi-supervised learning: predicting on unsupervised data...\n",
      "\u001b[1m1563/1563\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 78ms/step\n",
      "Confident positive samples found: 9305\n",
      "Confident negative samples found: 14528\n",
      "Retraining model with extended data...\n",
      "Epoch 1/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 551ms/step - accuracy: 0.9215 - loss: 0.2093 - val_accuracy: 0.9992 - val_loss: 0.0155\n",
      "Epoch 2/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m373s\u001b[0m 543ms/step - accuracy: 0.9282 - loss: 0.1891 - val_accuracy: 0.9965 - val_loss: 0.0349\n",
      "Epoch 3/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m374s\u001b[0m 545ms/step - accuracy: 0.9332 - loss: 0.1762 - val_accuracy: 0.9994 - val_loss: 0.0243\n",
      "Epoch 4/5\n",
      "\u001b[1m687/687\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m379s\u001b[0m 551ms/step - accuracy: 0.9341 - loss: 0.1689 - val_accuracy: 0.9971 - val_loss: 0.0451\n"
     ]
    }
   ],
   "source": [
    "# 10.Predict on all unsupervised data and use all confident predictions\n",
    "print(\"Semi-supervised learning: predicting on unsupervised data...\")\n",
    "\n",
    "unsup_seq = tokenizer.texts_to_sequences(unsup_cleaned)\n",
    "unsup_pad = pad_sequences(unsup_seq, maxlen=max_len, padding='post')\n",
    "\n",
    "unsup_preds = model.predict(unsup_pad).flatten()\n",
    "\n",
    "# Select all confident predictions\n",
    "conf_pos_idx = np.where(unsup_preds > 0.9)[0]\n",
    "conf_neg_idx = np.where(unsup_preds < 0.1)[0]\n",
    "\n",
    "print(f\"Confident positive samples found: {len(conf_pos_idx)}\")\n",
    "print(f\"Confident negative samples found: {len(conf_neg_idx)}\")\n",
    "\n",
    "# Add confident samples to training data\n",
    "new_texts = [unsup_cleaned[i] for i in np.concatenate([conf_pos_idx, conf_neg_idx])]\n",
    "new_labels = [1]*len(conf_pos_idx) + [0]*len(conf_neg_idx)\n",
    "\n",
    "new_seq = tokenizer.texts_to_sequences(new_texts)\n",
    "new_pad = pad_sequences(new_seq, maxlen=max_len, padding='post')\n",
    "new_labels = np.array(new_labels)\n",
    "\n",
    "X_train_pad = np.vstack([X_train_pad, new_pad])\n",
    "y_train = np.concatenate([y_train, new_labels])\n",
    "\n",
    "print(\"Retraining model with extended data...\")\n",
    "history2 = model.fit(\n",
    "    X_train_pad,\n",
    "    y_train,\n",
    "    validation_split=0.1,\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    callbacks=[EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "195a0b59-955a-44d2-9d1d-2cdd57953e11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating final model...\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 87ms/step - accuracy: 0.8662 - loss: 0.3436\n",
      "Test accuracy: 0.8638\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 84ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.87      0.86     12500\n",
      "           1       0.87      0.86      0.86     12500\n",
      "\n",
      "    accuracy                           0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 11.Evaluate model performance\n",
    "print(\"Evaluating final model...\")\n",
    "loss, accuracy = model.evaluate(X_test_pad, y_test)\n",
    "print(f\"Test accuracy: {accuracy:.4f}\")\n",
    "\n",
    "y_pred = (model.predict(X_test_pad) > 0.5).astype(int)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a59944c7-9ead-4965-8e27-eb5e0701fde1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model and tokenizer...\n",
      "Saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# 12.Save the trained model and tokenizer for future use\n",
    "print(\"Saving model and tokenizer...\")\n",
    "model.save('sentiment_bilstm.h5')\n",
    "\n",
    "with open('tokenizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tokenizer, f)\n",
    "\n",
    "print(\"Saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "995e3a47-9ae8-4466-b679-d6605a664050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading saved model and tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# 13.Load model and tokenizer (use when re-running notebook later)\n",
    "print(\"Loading saved model and tokenizer...\")\n",
    "loaded_model = load_model('sentiment_bilstm.h5')\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as f:\n",
    "    loaded_tokenizer = pickle.load(f)\n",
    "\n",
    "print(\"Loaded successfully.\")\n",
    "\n",
    "\n",
    "# Function to predict sentiment of a custom text\n",
    "def predict_sentiment(text):\n",
    "    cleaned = clean_text(text)\n",
    "    seq = loaded_tokenizer.texts_to_sequences([cleaned])\n",
    "    pad_seq = pad_sequences(seq, maxlen=max_len, padding='post')\n",
    "    prediction = loaded_model.predict(pad_seq)[0][0]\n",
    "    sentiment = \"Positive\" if prediction > 0.5 else \"Negative\"\n",
    "    return sentiment, prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ff2c98a-ff8c-4bb1-bb5b-1015b52088c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 850ms/step\n",
      "Review: The movie was absolutely fantastic! Great acting and story.\n",
      "Predicted Sentiment: Positive (Score: 0.9939)\n",
      "\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
      "Review: This was the worst film I have ever seen. Total waste of time.\n",
      "Predicted Sentiment: Negative (Score: 0.0019)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 14.Example predictions\n",
    "examples = [\n",
    "    \"The movie was absolutely fantastic! Great acting and story.\",\n",
    "    \"This was the worst film I have ever seen. Total waste of time.\"\n",
    "]\n",
    "\n",
    "for ex in examples:\n",
    "    sentiment, score = predict_sentiment(ex)\n",
    "    print(f\"Review: {ex}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment} (Score: {score:.4f})\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84be40d4-6f4a-41a6-9469-b1fc8c36aae9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
